{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1463aadf",
   "metadata": {},
   "source": [
    "## Deep svdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b897df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Args: {'num_epochs': 50, 'num_epochs_ae': 50, 'lr': 0.001, 'lr_ae': 0.001, 'weight_decay': 5e-07, 'weight_decay_ae': 0.005, 'lr_milestones': [50], 'batch_size': 1024, 'pretrain': True, 'latent_dim': 32, 'normal_class': 0}\n"
     ]
    }
   ],
   "source": [
    "# Deep SVDD 모듈 import\n",
    "import sys\n",
    "import os\n",
    "# deep_svdd 모듈에서 필요한 클래스와 함수 import\n",
    "from deep_svdd import (\n",
    "    get_mnist, \n",
    "    TrainerDeepSVDD, \n",
    "    eval,\n",
    "    device,\n",
    "    args\n",
    ")\n",
    "\n",
    "\n",
    "# 현재 노트북과 같은 디렉토리의 deep_svdd.py를 import하기 위한 경로 설정\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Args: {args}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abea458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# weights 디렉토리 생성 (pretrained weights 저장용)\n",
    "import os\n",
    "weights_dir = '../weights'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.makedirs(weights_dir)\n",
    "    print(f\"Created directory: {weights_dir}\")\n",
    "else:\n",
    "    is_pretrained = True\n",
    "    print(f\"Directory already exists: {weights_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d5a78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 6\n",
      "Test batches: 10\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loader 불러오기\n",
    "dataloader_train, dataloader_test = get_mnist(args, data_dir='../data/')\n",
    "print(f\"Train batches: {len(dataloader_train)}\")\n",
    "print(f\"Test batches: {len(dataloader_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828a5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerDeepSVDD initialized\n"
     ]
    }
   ],
   "source": [
    "# Network 학습준비, 구조 불러오기\n",
    "deep_SVDD = TrainerDeepSVDD(args, dataloader_train, device)\n",
    "print(\"TrainerDeepSVDD initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d357c0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining skipped (args.pretrain=False)\n"
     ]
    }
   ],
   "source": [
    "# DeepSVDD를 위한 DeepLearning pretrain 모델로 Weight 학습\n",
    "if args.pretrain and not is_pretrained:\n",
    "    print(\"Starting pretraining...\")\n",
    "    deep_SVDD.pretrain()\n",
    "    print(\"Pretraining completed!\")\n",
    "else:\n",
    "    print(\"Pretraining skipped (args.pretrain=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f863f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep SVDD training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep SVDD... Epoch: 0, Loss: 1.358\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.330\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.148\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.096\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.067\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.050\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.041\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.034\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.031\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.025\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.022\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.020\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.018\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.016\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.015\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.016\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.014\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.013\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.013\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.011\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.011\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.011\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.010\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.010\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.009\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.009\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.013\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.013\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.010\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.009\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.008\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.008\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.008\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.008\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.009\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.006\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.006\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.006\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.007\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.009\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.010\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.008\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# 학습된 가중치로 Deep_SVDD모델 Train\n",
    "print(\"Starting Deep SVDD training...\")\n",
    "net, c = deep_SVDD.train()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e9b6557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 96.60\n",
      "Evaluation completed!\n",
      "Labels shape: (10000,)\n",
      "Scores shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "labels, scores = eval(net, c, dataloader_test, device)\n",
    "print(\"Evaluation completed!\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8add8be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Threshold: 0.1 ---\n",
      "AUC: 0.9660\n",
      "FPR (at threshold=0.1): 0.1866\n",
      "TPR (at threshold=0.1): 0.7997\n",
      "최대 TPR: 1.0000\n",
      "최소 FPR: 0.0000\n",
      "f1 score (at threshold=0.1): 0.8855\n",
      "\n",
      "--- Threshold: 0.08 ---\n",
      "AUC: 0.9660\n",
      "FPR (at threshold=0.08): 0.1034\n",
      "TPR (at threshold=0.08): 0.8959\n",
      "최대 TPR: 1.0000\n",
      "최소 FPR: 0.0000\n",
      "f1 score (at threshold=0.08): 0.9399\n",
      "\n",
      "--- Threshold: 0.05 ---\n",
      "AUC: 0.9660\n",
      "FPR (at threshold=0.05): 0.0339\n",
      "TPR (at threshold=0.05): 0.9880\n",
      "최대 TPR: 1.0000\n",
      "최소 FPR: 0.0000\n",
      "f1 score (at threshold=0.05): 0.9813\n",
      "\n",
      "--- Threshold: 0.01 ---\n",
      "AUC: 0.9660\n",
      "FPR (at threshold=0.01): 0.0980\n",
      "TPR (at threshold=0.01): 1.0000\n",
      "최대 TPR: 1.0000\n",
      "최소 FPR: 0.0000\n",
      "f1 score (at threshold=0.01): 0.9485\n"
     ]
    }
   ],
   "source": [
    "# 각종 metric 계산\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# AUC 계산\n",
    "auc = roc_auc_score(labels, scores)\n",
    "# ROC Curve로 FPR, TPR 계산 (임곗값별, 여기선 임계값=0.5에서의 값 예시)\n",
    "fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "\n",
    "for thresh in [ 0.1,0.08, 0.05, 0.01]:\n",
    "    binary_pred = (scores >= thresh).astype(int)\n",
    "    f1 = f1_score(labels, binary_pred)\n",
    "    fpr_value = (binary_pred != labels).sum() / len(labels)\n",
    "    tpr_value = ((labels == 1) & (binary_pred == 1)).sum() / (labels == 1).sum()\n",
    "    print(f\"\\n--- Threshold: {thresh} ---\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"FPR (at threshold={thresh}): {fpr_value:.4f}\")\n",
    "    print(f\"TPR (at threshold={thresh}): {tpr_value:.4f}\")\n",
    "    print(f\"최대 TPR: {np.max(tpr):.4f}\")\n",
    "    print(f\"최소 FPR: {np.min(fpr):.4f}\")\n",
    "    print(f\"f1 score (at threshold={thresh}): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970251a6",
   "metadata": {},
   "source": [
    "## Deep SVDD Ablation Study\n",
    "정규화 계수 $\\nu$, 학습률, 임베딩 차원 $p$의 조합을 대상으로 Deep SVDD 성능을 비교합니다. 여기서 $\\nu$는 구현상 Adam의 weight decay 항으로 적용해 구(球) 학습 중 정규화를 조절합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce3a7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/9] nu=1e-04, latent_dim=32\n",
      "Training Deep SVDD... Epoch: 0, Loss: 17.590\n",
      "Training Deep SVDD... Epoch: 1, Loss: 2.769\n",
      "Training Deep SVDD... Epoch: 2, Loss: 1.227\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.745\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.438\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.267\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.205\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.171\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.140\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.119\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.106\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.096\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.088\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.081\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.076\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.072\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.068\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.064\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.061\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.059\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.056\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.052\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.050\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.048\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.047\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.045\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.044\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.042\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.041\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.040\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.038\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.037\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.036\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.035\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.034\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.033\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.033\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.032\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.031\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.030\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.029\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.029\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.028\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.026\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.025\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.025\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.024\n",
      "Testing...\n",
      "ROC AUC score: 98.05\n",
      "Thresh=0.100 | AUC: 98.05 | F1: 0.784 | TPR: 0.651 | FPR: 0.013\n",
      "Thresh=0.080 | AUC: 98.05 | F1: 0.870 | TPR: 0.776 | FPR: 0.027\n",
      "Thresh=0.050 | AUC: 98.05 | F1: 0.971 | TPR: 0.953 | FPR: 0.084\n",
      "Thresh=0.010 | AUC: 98.05 | F1: 0.955 | TPR: 1.000 | FPR: 0.701\n",
      "\n",
      "[2/9] nu=1e-04, latent_dim=64\n",
      "Training Deep SVDD... Epoch: 0, Loss: 24.702\n",
      "Training Deep SVDD... Epoch: 1, Loss: 4.489\n",
      "Training Deep SVDD... Epoch: 2, Loss: 2.178\n",
      "Training Deep SVDD... Epoch: 3, Loss: 1.108\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.821\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.536\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.405\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.326\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.264\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.226\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.198\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.176\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.161\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.149\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.139\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.130\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.122\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.115\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.109\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.104\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.099\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.095\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.091\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.088\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.085\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.081\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.079\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.076\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.073\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.071\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.069\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.067\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.065\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.063\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.061\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.059\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.058\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.056\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.055\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.052\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.051\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.050\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.049\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.048\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.047\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.046\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.044\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.044\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.043\n",
      "Testing...\n",
      "ROC AUC score: 99.21\n",
      "Thresh=0.100 | AUC: 99.21 | F1: 0.977 | TPR: 0.960 | FPR: 0.041\n",
      "Thresh=0.080 | AUC: 99.21 | F1: 0.990 | TPR: 0.988 | FPR: 0.071\n",
      "Thresh=0.050 | AUC: 99.21 | F1: 0.986 | TPR: 1.000 | FPR: 0.274\n",
      "Thresh=0.010 | AUC: 99.21 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n",
      "\n",
      "[3/9] nu=1e-04, latent_dim=128\n",
      "Training Deep SVDD... Epoch: 0, Loss: 74.835\n",
      "Training Deep SVDD... Epoch: 1, Loss: 16.592\n",
      "Training Deep SVDD... Epoch: 2, Loss: 5.501\n",
      "Training Deep SVDD... Epoch: 3, Loss: 3.035\n",
      "Training Deep SVDD... Epoch: 4, Loss: 2.007\n",
      "Training Deep SVDD... Epoch: 5, Loss: 1.430\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.909\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.743\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.594\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.496\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.446\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.400\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.367\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.341\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.318\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.300\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.283\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.269\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.257\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.244\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.234\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.224\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.215\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.206\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.199\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.191\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.185\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.183\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.173\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.167\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.162\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.158\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.153\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.147\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.143\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.139\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.135\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.132\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.128\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.124\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.121\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.118\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.116\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.113\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.111\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.107\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.106\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.103\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.101\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.098\n",
      "Testing...\n",
      "ROC AUC score: 98.67\n",
      "Thresh=0.100 | AUC: 98.67 | F1: 0.981 | TPR: 0.999 | FPR: 0.345\n",
      "Thresh=0.080 | AUC: 98.67 | F1: 0.973 | TPR: 1.000 | FPR: 0.524\n",
      "Thresh=0.050 | AUC: 98.67 | F1: 0.956 | TPR: 1.000 | FPR: 1.000\n",
      "Thresh=0.010 | AUC: 98.67 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n",
      "\n",
      "[4/9] nu=1e-03, latent_dim=32\n",
      "Training Deep SVDD... Epoch: 0, Loss: 17.142\n",
      "Training Deep SVDD... Epoch: 1, Loss: 2.858\n",
      "Training Deep SVDD... Epoch: 2, Loss: 1.068\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.636\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.318\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.252\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.181\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.132\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.111\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.095\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.080\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.074\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.067\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.062\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.058\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.051\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.049\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.046\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.044\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.043\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.041\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.039\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.038\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.037\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.035\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.034\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.033\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.032\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.031\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.030\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.030\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.029\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.028\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.026\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.026\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.025\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.024\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.024\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.023\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.023\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.022\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.022\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.021\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.021\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.021\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.020\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.020\n",
      "Testing...\n",
      "ROC AUC score: 96.65\n",
      "Thresh=0.100 | AUC: 96.65 | F1: 0.600 | TPR: 0.417 | FPR: 0.003\n",
      "Thresh=0.080 | AUC: 96.65 | F1: 0.708 | TPR: 0.533 | FPR: 0.008\n",
      "Thresh=0.050 | AUC: 96.65 | F1: 0.890 | TPR: 0.806 | FPR: 0.045\n",
      "Thresh=0.010 | AUC: 96.65 | F1: 0.956 | TPR: 1.000 | FPR: 0.843\n",
      "\n",
      "[5/9] nu=1e-03, latent_dim=64\n",
      "Training Deep SVDD... Epoch: 0, Loss: 34.975\n",
      "Training Deep SVDD... Epoch: 1, Loss: 7.244\n",
      "Training Deep SVDD... Epoch: 2, Loss: 3.098\n",
      "Training Deep SVDD... Epoch: 3, Loss: 1.397\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.794\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.620\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.466\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.367\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.288\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.248\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.219\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.196\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.177\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.163\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.151\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.141\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.132\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.125\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.118\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.112\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.107\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.102\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.098\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.094\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.090\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.087\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.084\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.081\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.078\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.075\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.073\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.070\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.068\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.067\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.065\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.063\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.061\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.059\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.058\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.056\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.053\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.052\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.051\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.049\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.048\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.047\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.046\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.045\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.045\n",
      "Testing...\n",
      "ROC AUC score: 98.40\n",
      "Thresh=0.100 | AUC: 98.40 | F1: 0.967 | TPR: 0.943 | FPR: 0.069\n",
      "Thresh=0.080 | AUC: 98.40 | F1: 0.983 | TPR: 0.980 | FPR: 0.119\n",
      "Thresh=0.050 | AUC: 98.40 | F1: 0.983 | TPR: 0.999 | FPR: 0.313\n",
      "Thresh=0.010 | AUC: 98.40 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n",
      "\n",
      "[6/9] nu=1e-03, latent_dim=128\n",
      "Training Deep SVDD... Epoch: 0, Loss: 80.664\n",
      "Training Deep SVDD... Epoch: 1, Loss: 15.461\n",
      "Training Deep SVDD... Epoch: 2, Loss: 4.405\n",
      "Training Deep SVDD... Epoch: 3, Loss: 2.212\n",
      "Training Deep SVDD... Epoch: 4, Loss: 1.605\n",
      "Training Deep SVDD... Epoch: 5, Loss: 1.247\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.865\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.625\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.495\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.410\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.356\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.314\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.282\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.260\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.244\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.228\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.216\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.205\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.196\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.187\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.179\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.172\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.165\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.159\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.154\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.148\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.144\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.140\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.134\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.131\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.126\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.124\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.119\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.115\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.113\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.110\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.106\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.104\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.101\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.098\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.096\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.095\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.093\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.091\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.089\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.087\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.084\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.084\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.082\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.079\n",
      "Testing...\n",
      "ROC AUC score: 96.61\n",
      "Thresh=0.100 | AUC: 96.61 | F1: 0.975 | TPR: 0.971 | FPR: 0.195\n",
      "Thresh=0.080 | AUC: 96.61 | F1: 0.978 | TPR: 0.990 | FPR: 0.319\n",
      "Thresh=0.050 | AUC: 96.61 | F1: 0.966 | TPR: 0.999 | FPR: 0.632\n",
      "Thresh=0.010 | AUC: 96.61 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n",
      "\n",
      "[7/9] nu=1e-02, latent_dim=32\n",
      "Training Deep SVDD... Epoch: 0, Loss: 23.940\n",
      "Training Deep SVDD... Epoch: 1, Loss: 4.300\n",
      "Training Deep SVDD... Epoch: 2, Loss: 1.556\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.712\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.538\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.371\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.257\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.208\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.166\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.140\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.123\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.112\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.102\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.096\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.089\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.084\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.079\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.075\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.071\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.067\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.064\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.061\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.059\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.057\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.052\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.050\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.048\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.047\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.045\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.044\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.043\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.041\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.040\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.039\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.038\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.036\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.035\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.035\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.033\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.032\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.032\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.031\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.030\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.029\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.028\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.028\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.027\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.026\n",
      "Testing...\n",
      "ROC AUC score: 96.25\n",
      "Thresh=0.100 | AUC: 96.25 | F1: 0.782 | TPR: 0.641 | FPR: 0.030\n",
      "Thresh=0.080 | AUC: 96.25 | F1: 0.857 | TPR: 0.751 | FPR: 0.048\n",
      "Thresh=0.050 | AUC: 96.25 | F1: 0.956 | TPR: 0.929 | FPR: 0.133\n",
      "Thresh=0.010 | AUC: 96.25 | F1: 0.955 | TPR: 1.000 | FPR: 0.749\n",
      "\n",
      "[8/9] nu=1e-02, latent_dim=64\n",
      "Training Deep SVDD... Epoch: 0, Loss: 47.535\n",
      "Training Deep SVDD... Epoch: 1, Loss: 9.656\n",
      "Training Deep SVDD... Epoch: 2, Loss: 3.077\n",
      "Training Deep SVDD... Epoch: 3, Loss: 1.604\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.983\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.761\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.522\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.424\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.326\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.279\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.240\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.211\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.189\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.173\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.159\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.148\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.138\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.131\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.123\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.117\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.112\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.107\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.102\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.098\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.094\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.091\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.087\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.084\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.081\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.079\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.076\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.074\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.072\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.069\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.068\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.066\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.064\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.062\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.061\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.059\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.058\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.056\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.055\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.054\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.053\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.052\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.051\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.050\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.048\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.047\n",
      "Testing...\n",
      "ROC AUC score: 98.55\n",
      "Thresh=0.100 | AUC: 98.55 | F1: 0.972 | TPR: 0.953 | FPR: 0.061\n",
      "Thresh=0.080 | AUC: 98.55 | F1: 0.983 | TPR: 0.979 | FPR: 0.115\n",
      "Thresh=0.050 | AUC: 98.55 | F1: 0.981 | TPR: 0.998 | FPR: 0.335\n",
      "Thresh=0.010 | AUC: 98.55 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n",
      "\n",
      "[9/9] nu=1e-02, latent_dim=128\n",
      "Training Deep SVDD... Epoch: 0, Loss: 92.425\n",
      "Training Deep SVDD... Epoch: 1, Loss: 20.460\n",
      "Training Deep SVDD... Epoch: 2, Loss: 6.463\n",
      "Training Deep SVDD... Epoch: 3, Loss: 2.909\n",
      "Training Deep SVDD... Epoch: 4, Loss: 1.810\n",
      "Training Deep SVDD... Epoch: 5, Loss: 1.210\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.933\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.728\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.600\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.520\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.468\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.423\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.389\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.362\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.339\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.320\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.301\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.286\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.270\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.257\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.244\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.233\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.223\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.214\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.206\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.195\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.187\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.180\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.174\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.166\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.160\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.152\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.147\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.141\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.137\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.132\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.127\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.123\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.121\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.116\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.112\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.109\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.107\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.103\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.101\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.098\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.096\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.095\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.091\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.088\n",
      "Testing...\n",
      "ROC AUC score: 96.02\n",
      "Thresh=0.100 | AUC: 96.02 | F1: 0.972 | TPR: 0.996 | FPR: 0.491\n",
      "Thresh=0.080 | AUC: 96.02 | F1: 0.966 | TPR: 0.999 | FPR: 0.641\n",
      "Thresh=0.050 | AUC: 96.02 | F1: 0.954 | TPR: 1.000 | FPR: 0.811\n",
      "Thresh=0.010 | AUC: 96.02 | F1: 0.948 | TPR: 1.000 | FPR: 1.000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nu</th>\n",
       "      <th>latent_dim</th>\n",
       "      <th>thresh</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.985518</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.274490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.987916</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.977311</td>\n",
       "      <td>0.960421</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986738</td>\n",
       "      <td>0.980633</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.344898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nu  latent_dim  thresh   roc_auc        f1       tpr       fpr\n",
       "0  0.0001          64    0.01  0.992125  0.948475  1.000000  1.000000\n",
       "1  0.0001          64    0.05  0.992125  0.985518  0.999667  0.274490\n",
       "2  0.0001          64    0.08  0.992125  0.990000  0.987916  0.071429\n",
       "3  0.0001          64    0.10  0.992125  0.977311  0.960421  0.040816\n",
       "4  0.0001         128    0.10  0.986738  0.980633  0.999224  0.344898"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 하이퍼파라미터 값 정의\n",
    "nu_values = [1e-4, 1e-3, 1e-2]\n",
    "latent_dims = [32, 64, 128]\n",
    "threshs = [0.1, 0.08, 0.05, 0.01]\n",
    "\n",
    "ablation_records = []\n",
    "total_runs = len(nu_values) * len(latent_dims)  \n",
    "\n",
    "run_idx = 0\n",
    "\n",
    "# nu, lr, latent_dim에 따라 결과 기록\n",
    "for nu,  latent_dim in itertools.product(nu_values,  latent_dims):\n",
    "    run_idx += 1\n",
    "    print(f\"\\n[{run_idx}/{total_runs}] nu={nu:.0e}, latent_dim={latent_dim}\")\n",
    "    run_args = copy.deepcopy(args)\n",
    "    run_args.weight_decay = nu\n",
    "    run_args.lr = 0.001\n",
    "    run_args.latent_dim = latent_dim\n",
    "    run_args.pretrain = False  # latent_dim이 달라질 때는 사전학습 가중치를 사용할 수 없음\n",
    "\n",
    "    trainer = TrainerDeepSVDD(run_args, dataloader_train, device)\n",
    "    net, c = trainer.train()\n",
    "    labels, scores = eval(net, c, dataloader_test, device)\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    fpr_arr, tpr_arr, roc_thresholds = roc_curve(labels, scores)\n",
    "    for thresh in threshs:\n",
    "        binary_pred = (scores >= thresh).astype(int)\n",
    "        f1 = f1_score(labels, binary_pred)\n",
    "        # threshold와 가장 가까운 위치 찾기\n",
    "        thresh_idx = np.argmin(np.abs(roc_thresholds - thresh))\n",
    "        tpr_at_thresh = tpr_arr[thresh_idx]\n",
    "        fpr_at_thresh = fpr_arr[thresh_idx]\n",
    "\n",
    "        ablation_records.append({\n",
    "            'nu': nu,\n",
    "            'latent_dim': latent_dim,\n",
    "            'thresh': thresh,\n",
    "            'roc_auc': auc,\n",
    "            'f1': f1,\n",
    "            'tpr': tpr_at_thresh,\n",
    "            'fpr': fpr_at_thresh\n",
    "        })\n",
    "        print(f\"Thresh={thresh:.3f} | AUC: {auc*100:.2f} | F1: {f1:.3f} | TPR: {tpr_at_thresh:.3f} | FPR: {fpr_at_thresh:.3f}\")\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_records)\n",
    "ablation_df = ablation_df.sort_values('roc_auc', ascending=False).reset_index(drop=True)\n",
    "ablation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1e1ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC setting | nu=1e-03, thresh=1.0e-01, p=32, AUC=96.65, F1=0.600, TPR=41.72, FPR=0.31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nu</th>\n",
       "      <th>latent_dim</th>\n",
       "      <th>thresh</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>roc_auc_pct</th>\n",
       "      <th>tpr_pct</th>\n",
       "      <th>fpr_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.987916</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>99.212476</td>\n",
       "      <td>98.791574</td>\n",
       "      <td>7.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.985518</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.274490</td>\n",
       "      <td>99.212476</td>\n",
       "      <td>99.966741</td>\n",
       "      <td>27.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.977311</td>\n",
       "      <td>0.960421</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>99.212476</td>\n",
       "      <td>96.042129</td>\n",
       "      <td>4.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.992125</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.212476</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986738</td>\n",
       "      <td>0.980633</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.344898</td>\n",
       "      <td>98.673775</td>\n",
       "      <td>99.922395</td>\n",
       "      <td>34.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.986738</td>\n",
       "      <td>0.972658</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.524490</td>\n",
       "      <td>98.673775</td>\n",
       "      <td>99.977827</td>\n",
       "      <td>52.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.986738</td>\n",
       "      <td>0.956167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.673775</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.986738</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.673775</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.985470</td>\n",
       "      <td>0.983302</td>\n",
       "      <td>0.979268</td>\n",
       "      <td>0.115306</td>\n",
       "      <td>98.547038</td>\n",
       "      <td>97.926829</td>\n",
       "      <td>11.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.985470</td>\n",
       "      <td>0.981089</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.334694</td>\n",
       "      <td>98.547038</td>\n",
       "      <td>99.800443</td>\n",
       "      <td>33.469388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.985470</td>\n",
       "      <td>0.972153</td>\n",
       "      <td>0.952550</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>98.547038</td>\n",
       "      <td>95.254989</td>\n",
       "      <td>6.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.985470</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.547038</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.983976</td>\n",
       "      <td>0.983253</td>\n",
       "      <td>0.979712</td>\n",
       "      <td>0.119388</td>\n",
       "      <td>98.397642</td>\n",
       "      <td>97.971175</td>\n",
       "      <td>11.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.983976</td>\n",
       "      <td>0.982613</td>\n",
       "      <td>0.999335</td>\n",
       "      <td>0.313265</td>\n",
       "      <td>98.397642</td>\n",
       "      <td>99.933481</td>\n",
       "      <td>31.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.983976</td>\n",
       "      <td>0.967331</td>\n",
       "      <td>0.942905</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>98.397642</td>\n",
       "      <td>94.290466</td>\n",
       "      <td>6.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.983976</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.397642</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.980492</td>\n",
       "      <td>0.970886</td>\n",
       "      <td>0.953215</td>\n",
       "      <td>0.083673</td>\n",
       "      <td>98.049154</td>\n",
       "      <td>95.321508</td>\n",
       "      <td>8.367347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.980492</td>\n",
       "      <td>0.954902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701020</td>\n",
       "      <td>98.049154</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>70.102041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.980492</td>\n",
       "      <td>0.870380</td>\n",
       "      <td>0.775721</td>\n",
       "      <td>0.026531</td>\n",
       "      <td>98.049154</td>\n",
       "      <td>77.572062</td>\n",
       "      <td>2.653061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.980492</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.651330</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>98.049154</td>\n",
       "      <td>65.133038</td>\n",
       "      <td>1.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.956107</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>96.646851</td>\n",
       "      <td>99.977827</td>\n",
       "      <td>84.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.890032</td>\n",
       "      <td>0.805765</td>\n",
       "      <td>0.044898</td>\n",
       "      <td>96.646851</td>\n",
       "      <td>80.576497</td>\n",
       "      <td>4.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.708256</td>\n",
       "      <td>0.533370</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>96.646851</td>\n",
       "      <td>53.337029</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.599783</td>\n",
       "      <td>0.417184</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>96.646851</td>\n",
       "      <td>41.718404</td>\n",
       "      <td>0.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.977929</td>\n",
       "      <td>0.989800</td>\n",
       "      <td>0.319388</td>\n",
       "      <td>96.611430</td>\n",
       "      <td>98.980044</td>\n",
       "      <td>31.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.974676</td>\n",
       "      <td>0.970732</td>\n",
       "      <td>0.194898</td>\n",
       "      <td>96.611430</td>\n",
       "      <td>97.073171</td>\n",
       "      <td>19.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.966445</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.631633</td>\n",
       "      <td>96.611430</td>\n",
       "      <td>99.944568</td>\n",
       "      <td>63.163265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.611430</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.955897</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>96.254978</td>\n",
       "      <td>92.871397</td>\n",
       "      <td>13.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.955458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.748980</td>\n",
       "      <td>96.254978</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.897959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.751109</td>\n",
       "      <td>0.047959</td>\n",
       "      <td>96.254978</td>\n",
       "      <td>75.110865</td>\n",
       "      <td>4.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.962550</td>\n",
       "      <td>0.781608</td>\n",
       "      <td>0.641131</td>\n",
       "      <td>0.029592</td>\n",
       "      <td>96.254978</td>\n",
       "      <td>64.113082</td>\n",
       "      <td>2.959184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.960174</td>\n",
       "      <td>0.972195</td>\n",
       "      <td>0.996231</td>\n",
       "      <td>0.490816</td>\n",
       "      <td>96.017433</td>\n",
       "      <td>99.623060</td>\n",
       "      <td>49.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.960174</td>\n",
       "      <td>0.966016</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.640816</td>\n",
       "      <td>96.017433</td>\n",
       "      <td>99.900222</td>\n",
       "      <td>64.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.960174</td>\n",
       "      <td>0.954144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>96.017433</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>81.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.960174</td>\n",
       "      <td>0.948475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.017433</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nu  latent_dim  thresh   roc_auc        f1       tpr       fpr  \\\n",
       "0   0.0001          64    0.08  0.992125  0.990000  0.987916  0.071429   \n",
       "1   0.0001          64    0.05  0.992125  0.985518  0.999667  0.274490   \n",
       "2   0.0001          64    0.10  0.992125  0.977311  0.960421  0.040816   \n",
       "3   0.0001          64    0.01  0.992125  0.948475  1.000000  1.000000   \n",
       "4   0.0001         128    0.10  0.986738  0.980633  0.999224  0.344898   \n",
       "5   0.0001         128    0.08  0.986738  0.972658  0.999778  0.524490   \n",
       "6   0.0001         128    0.05  0.986738  0.956167  1.000000  1.000000   \n",
       "7   0.0001         128    0.01  0.986738  0.948475  1.000000  1.000000   \n",
       "8   0.0100          64    0.08  0.985470  0.983302  0.979268  0.115306   \n",
       "9   0.0100          64    0.05  0.985470  0.981089  0.998004  0.334694   \n",
       "10  0.0100          64    0.10  0.985470  0.972153  0.952550  0.061224   \n",
       "11  0.0100          64    0.01  0.985470  0.948475  1.000000  1.000000   \n",
       "12  0.0010          64    0.08  0.983976  0.983253  0.979712  0.119388   \n",
       "13  0.0010          64    0.05  0.983976  0.982613  0.999335  0.313265   \n",
       "14  0.0010          64    0.10  0.983976  0.967331  0.942905  0.069388   \n",
       "15  0.0010          64    0.01  0.983976  0.948475  1.000000  1.000000   \n",
       "16  0.0001          32    0.05  0.980492  0.970886  0.953215  0.083673   \n",
       "17  0.0001          32    0.01  0.980492  0.954902  1.000000  0.701020   \n",
       "18  0.0001          32    0.08  0.980492  0.870380  0.775721  0.026531   \n",
       "19  0.0001          32    0.10  0.980492  0.783926  0.651330  0.013265   \n",
       "20  0.0010          32    0.01  0.966469  0.956107  0.999778  0.842857   \n",
       "21  0.0010          32    0.05  0.966469  0.890032  0.805765  0.044898   \n",
       "22  0.0010          32    0.08  0.966469  0.708256  0.533370  0.008163   \n",
       "23  0.0010          32    0.10  0.966469  0.599783  0.417184  0.003061   \n",
       "24  0.0010         128    0.08  0.966114  0.977929  0.989800  0.319388   \n",
       "25  0.0010         128    0.10  0.966114  0.974676  0.970732  0.194898   \n",
       "26  0.0010         128    0.05  0.966114  0.966445  0.999446  0.631633   \n",
       "27  0.0010         128    0.01  0.966114  0.948475  1.000000  1.000000   \n",
       "28  0.0100          32    0.05  0.962550  0.955897  0.928714  0.132653   \n",
       "29  0.0100          32    0.01  0.962550  0.955458  1.000000  0.748980   \n",
       "30  0.0100          32    0.08  0.962550  0.856693  0.751109  0.047959   \n",
       "31  0.0100          32    0.10  0.962550  0.781608  0.641131  0.029592   \n",
       "32  0.0100         128    0.10  0.960174  0.972195  0.996231  0.490816   \n",
       "33  0.0100         128    0.08  0.960174  0.966016  0.999002  0.640816   \n",
       "34  0.0100         128    0.05  0.960174  0.954144  1.000000  0.811224   \n",
       "35  0.0100         128    0.01  0.960174  0.948475  1.000000  1.000000   \n",
       "\n",
       "    roc_auc_pct     tpr_pct     fpr_pct  \n",
       "0     99.212476   98.791574    7.142857  \n",
       "1     99.212476   99.966741   27.448980  \n",
       "2     99.212476   96.042129    4.081633  \n",
       "3     99.212476  100.000000  100.000000  \n",
       "4     98.673775   99.922395   34.489796  \n",
       "5     98.673775   99.977827   52.448980  \n",
       "6     98.673775  100.000000  100.000000  \n",
       "7     98.673775  100.000000  100.000000  \n",
       "8     98.547038   97.926829   11.530612  \n",
       "9     98.547038   99.800443   33.469388  \n",
       "10    98.547038   95.254989    6.122449  \n",
       "11    98.547038  100.000000  100.000000  \n",
       "12    98.397642   97.971175   11.938776  \n",
       "13    98.397642   99.933481   31.326531  \n",
       "14    98.397642   94.290466    6.938776  \n",
       "15    98.397642  100.000000  100.000000  \n",
       "16    98.049154   95.321508    8.367347  \n",
       "17    98.049154  100.000000   70.102041  \n",
       "18    98.049154   77.572062    2.653061  \n",
       "19    98.049154   65.133038    1.326531  \n",
       "20    96.646851   99.977827   84.285714  \n",
       "21    96.646851   80.576497    4.489796  \n",
       "22    96.646851   53.337029    0.816327  \n",
       "23    96.646851   41.718404    0.306122  \n",
       "24    96.611430   98.980044   31.938776  \n",
       "25    96.611430   97.073171   19.489796  \n",
       "26    96.611430   99.944568   63.163265  \n",
       "27    96.611430  100.000000  100.000000  \n",
       "28    96.254978   92.871397   13.265306  \n",
       "29    96.254978  100.000000   74.897959  \n",
       "30    96.254978   75.110865    4.795918  \n",
       "31    96.254978   64.113082    2.959184  \n",
       "32    96.017433   99.623060   49.081633  \n",
       "33    96.017433   99.900222   64.081633  \n",
       "34    96.017433  100.000000   81.122449  \n",
       "35    96.017433  100.000000  100.000000  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config = ablation_df.sort_values(['fpr', 'tpr'], ascending=[True, False]).iloc[0]\n",
    "print(\n",
    "    f\"Best AUC setting | nu={best_config['nu']:.0e}, thresh={best_config['thresh']:.1e}, \"\n",
    "    f\"p={int(best_config['latent_dim'])}, AUC={best_config['roc_auc'] * 100:.2f}, \"\n",
    "    f\"F1={best_config['f1']:.3f}, TPR={best_config['tpr'] * 100:.2f}, FPR={best_config['fpr'] * 100:.2f}\"\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    ablation_df\n",
    "    .sort_values(['roc_auc', 'f1'], ascending=[False, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "summary_df.assign(\n",
    "    roc_auc_pct=lambda df: df['roc_auc'] * 100,\n",
    "    tpr_pct=lambda df: df['tpr'] * 100,\n",
    "    fpr_pct=lambda df: df['fpr'] * 100\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493baf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_deep_svdd_score = f\"{best_config['roc_auc']:.4f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70674c3f",
   "metadata": {},
   "source": [
    "## 비교 실험: KDE, GMM, LOF, PCA, AE, One-Class SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef1b2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# 결과 저장용 딕셔너리\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9fe963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, scores):\n",
    "    auc = roc_auc_score(y_true, scores)\n",
    "    fpr_curve, tpr_curve, thresholds = roc_curve(y_true, scores)\n",
    "    best_f1 = -np.inf\n",
    "    best_stats = {'tpr': 0.0, 'fpr': 0.0, 'threshold': thresholds[0] if len(thresholds) > 0 else 0.0}\n",
    "\n",
    "    for fpr_val, tpr_val, thresh in zip(fpr_curve, tpr_curve, thresholds):\n",
    "        preds = (scores >= thresh).astype(int)\n",
    "        current_f1 = f1_score(y_true, preds)\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_stats = {'tpr': tpr_val, 'fpr': fpr_val, 'threshold': thresh}\n",
    "\n",
    "    return {\n",
    "        'scores': scores,\n",
    "        'auc': auc,\n",
    "        'f1': best_f1,\n",
    "        'tpr': best_stats['tpr'],\n",
    "        'fpr': best_stats['fpr'],\n",
    "        'threshold': best_stats['threshold']\n",
    "    }\n",
    "\n",
    "\n",
    "def print_metrics(name, metrics):\n",
    "    print(\n",
    "        f\"{name} Metrics -> AUC: {metrics['auc']:.4f}, \"\n",
    "        f\"F1: {metrics['f1']:.3f}, \"\n",
    "        f\"TPR: {metrics['tpr'] * 100:.2f}%, \"\n",
    "        f\"FPR: {metrics['fpr'] * 100:.2f}%, \"\n",
    "        f\"thr={metrics['threshold']:.4f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01047a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_svdd import (\n",
    "    get_mnist_ex\n",
    ")\n",
    "data_dict = get_mnist_ex(args,  data_dir='../data/')\n",
    "# #data_dict\n",
    "# {\n",
    "#         \"x_train\": x_train,\n",
    "#         \"y_train\": y_train,\n",
    "#         \"x_test\": x_test,\n",
    "#         \"y_test\": y_test,\n",
    "#         \"dataloader_train\": dataloader_train,\n",
    "#         \"dataloader_test\": dataloader_test\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fd70ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jspar\\AppData\\Local\\Temp\\ipykernel_67416\\4255807394.py:7: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  X_train_np = np.array(X_train_raw)\n",
      "C:\\Users\\jspar\\AppData\\Local\\Temp\\ipykernel_67416\\4255807394.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  X_test_np = np.array(X_test_raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 정규화 (일부 알고리즘은 정규화가 필요)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_raw = data_dict[\"x_train\"]\n",
    "X_test_raw = data_dict[\"x_test\"]\n",
    "\n",
    "X_train_np = np.array(X_train_raw)\n",
    "X_test_np = np.array(X_test_raw)\n",
    "\n",
    "if X_train_np.ndim > 2:\n",
    "    X_train_np = X_train_np.reshape(X_train_np.shape[0], -1)\n",
    "if X_test_np.ndim > 2:\n",
    "    X_test_np = X_test_np.reshape(X_test_np.shape[0], -1)\n",
    "\n",
    "X_train_np = X_train_np.astype(np.float32) / 255.0\n",
    "X_test_np = X_test_np.astype(np.float32) / 255.0\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "X_test_scaled = scaler.transform(X_test_np)\n",
    "print(\"Data scaled successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8399a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. Training KDE...\n",
      "KDE Metrics -> AUC: 0.7727, F1: 0.950, TPR: 99.76%, FPR: 94.90%, thr=-508.1620\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score  # Add this import to fix NameError\n",
    "\n",
    "# 1. KDE (Kernel Density Estimation)\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Training KDE...\")\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2)\n",
    "kde.fit(X_train_scaled)\n",
    "\n",
    "# Test 데이터에 대한 log-likelihood 계산 (낮을수록 이상)\n",
    "kde_scores = -kde.score_samples(X_test_scaled)  # 음수로 변환하여 높을수록 이상\n",
    "results['KDE'] = compute_metrics(data_dict[\"y_test\"], kde_scores)\n",
    "print_metrics(\"KDE\", results['KDE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1e2c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "2. Training GMM...\n",
      "GMM Metrics -> AUC: 0.9419, F1: 0.978, TPR: 99.27%, FPR: 35.31%, thr=-1444.8498\n"
     ]
    }
   ],
   "source": [
    "# 2. GMM (Gaussian Mixture Model)\n",
    "print(\"=\" * 50)\n",
    "print(\"2. Training GMM...\")\n",
    "\n",
    "# 공분산 붕괴 방지를 위한 정규화 및 dtype 변환\n",
    "X_train_gmm = X_train_scaled.astype(np.float64, copy=False)\n",
    "X_test_gmm = X_test_scaled.astype(np.float64, copy=False)\n",
    "\n",
    "gmm = GaussianMixture(n_components=5, random_state=42, max_iter=200, reg_covar=1e-5)\n",
    "gmm.fit(X_train_gmm)\n",
    "\n",
    "# Test 데이터에 대한 log-likelihood 계산 (낮을수록 이상)\n",
    "gmm_scores = -gmm.score_samples(X_test_gmm)  # 음수로 변환하여 높을수록 이상\n",
    "results['GMM'] = compute_metrics(data_dict[\"y_test\"], gmm_scores)\n",
    "print_metrics(\"GMM\", results['GMM'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a1ec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "3. Training LOF...\n",
      "LOF Metrics -> AUC: 0.9310, F1: 0.979, TPR: 99.69%, FPR: 36.33%, thr=1.0910\n"
     ]
    }
   ],
   "source": [
    "# 3. LOF (Local Outlier Factor)\n",
    "print(\"=\" * 50)\n",
    "print(\"3. Training LOF...\")\n",
    "# LOF는 fit_predict를 사용하지만, anomaly_score_를 사용하여 점수 얻기\n",
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\n",
    "lof.fit(X_train_scaled)\n",
    "\n",
    "# Test 데이터에 대한 점수 계산 (높을수록 이상)\n",
    "lof_scores = -lof.score_samples(X_test_scaled)  # 음수로 변환하여 높을수록 이상\n",
    "results['LOF'] = compute_metrics(data_dict[\"y_test\"], lof_scores)\n",
    "print_metrics(\"LOF\", results['LOF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f5374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "4. Training PCA...\n",
      "PCA Metrics -> AUC: 0.9600, F1: 0.988, TPR: 99.32%, FPR: 15.10%, thr=0.2012\n"
     ]
    }
   ],
   "source": [
    "# 4. PCA (Principal Component Analysis)\n",
    "print(\"=\" * 50)\n",
    "print(\"4. Training PCA...\")\n",
    "n_components = 50  # 주성분 개수\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Test 데이터를 주성분 공간으로 변환 후 원래 공간으로 재구성\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "X_test_reconstructed = pca.inverse_transform(X_test_pca)\n",
    "\n",
    "# 재구성 오차 계산 (높을수록 이상)\n",
    "pca_scores = np.mean((X_test_scaled - X_test_reconstructed) ** 2, axis=1)\n",
    "results['PCA'] = compute_metrics(data_dict[\"y_test\"], pca_scores)\n",
    "print_metrics(\"PCA\", results['PCA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f395ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "5. Training AutoEncoder...\n",
      "Epoch [10/50], Loss: 0.388209\n",
      "Epoch [20/50], Loss: 0.318863\n",
      "Epoch [30/50], Loss: 0.272643\n",
      "Epoch [40/50], Loss: 0.243862\n",
      "Epoch [50/50], Loss: 0.223312\n",
      "AE Metrics -> AUC: 0.9645, F1: 0.990, TPR: 99.51%, FPR: 13.57%, thr=0.2409\n"
     ]
    }
   ],
   "source": [
    "# 5. AutoEncoder (AE)\n",
    "print(\"=\" * 50)\n",
    "print(\"5. Training AutoEncoder...\")\n",
    "\n",
    "import torch\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# AutoEncoder 학습\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "ae_model = AutoEncoder(input_dim, encoding_dim=32).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ae_model.parameters(), lr=0.001)\n",
    "\n",
    "# 데이터를 tensor로 변환\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# 학습\n",
    "ae_model.train()\n",
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch = X_train_tensor[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = ae_model(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(X_train_tensor)*batch_size:.6f}\")\n",
    "\n",
    "# Test 데이터에 대한 재구성 오차 계산\n",
    "ae_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_reconstructed = ae_model(X_test_tensor)\n",
    "    ae_scores = torch.mean((X_test_tensor - X_test_reconstructed) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "results['AE'] = compute_metrics(data_dict[\"y_test\"], ae_scores)\n",
    "print_metrics(\"AE\", results['AE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c17ac1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "6. Training One-Class SVM...\n",
      "One-Class SVM Metrics -> AUC: 0.9598, F1: 0.986, TPR: 99.15%, FPR: 17.45%, thr=-36.2972\n"
     ]
    }
   ],
   "source": [
    "# 6. One-Class SVM\n",
    "print(\"=\" * 50)\n",
    "print(\"6. Training One-Class SVM...\")\n",
    "ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)\n",
    "ocsvm.fit(X_train_scaled)\n",
    "\n",
    "# Test 데이터에 대한 점수 계산 (낮을수록 이상, 음수는 이상치)\n",
    "ocsvm_scores = -ocsvm.score_samples(X_test_scaled)  # 음수로 변환하여 높을수록 이상\n",
    "results['One-Class SVM'] = compute_metrics(data_dict[\"y_test\"], ocsvm_scores)\n",
    "print_metrics(\"One-Class SVM\", results['One-Class SVM'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc401dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm_model = globals().get(\"ocsvm\")\n",
    "ocsvm_kernel = getattr(ocsvm_model, \"kernel\", \"N/A\") if ocsvm_model else \"N/A\"\n",
    "ocsvm_gamma = getattr(ocsvm_model, \"gamma\", \"N/A\") if ocsvm_model else \"N/A\"\n",
    "ocsvm_nu = getattr(ocsvm_model, \"nu\", \"N/A\") if ocsvm_model else \"N/A\"\n",
    "\n",
    "hyperparam_rows = [\n",
    "    {\n",
    "        \"Method\": \"Deep SVDD\",\n",
    "        \"Key Hyperparameters\": (\n",
    "            f\"nu={best_config['nu']:.0e}, lr={best_config['lr']:.1e}, \"\n",
    "            f\"latent_dim={int(best_config['latent_dim'])}, pretrain={args.pretrain}\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"KDE\",\n",
    "        \"Key Hyperparameters\": f\"kernel={kde.kernel}, bandwidth={kde.bandwidth}\",\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"GMM\",\n",
    "        \"Key Hyperparameters\": (\n",
    "            f\"n_components={gmm.n_components}, covariance_type={gmm.covariance_type}, \"\n",
    "            f\"reg_covar={gmm.reg_covar}\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"LOF\",\n",
    "        \"Key Hyperparameters\": (\n",
    "            f\"n_neighbors={lof.n_neighbors}, contamination={lof.contamination}, \"\n",
    "            f\"novelty={lof.novelty}\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"PCA\",\n",
    "        \"Key Hyperparameters\": f\"n_components={n_components}\",\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"AutoEncoder\",\n",
    "        \"Key Hyperparameters\": (\n",
    "            f\"encoding_dim=32, hidden_layers=[128,64], epochs={n_epochs}, \"\n",
    "            f\"lr=0.001, batch_size={batch_size}\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"One-Class SVM\",\n",
    "        \"Key Hyperparameters\": (\n",
    "            f\"kernel={ocsvm_kernel}, gamma={ocsvm_gamma}, nu={ocsvm_nu}\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "hyperparam_df = pd.DataFrame(hyperparam_rows)\n",
    "hyperparam_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35a7936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON RESULTS\n",
      "======================================================================\n",
      "       Method ROC AUC    F1 TPR (%) FPR (%)\n",
      "           AE  0.9645 0.990   99.51   13.57\n",
      "          PCA  0.9600 0.988   99.32   15.10\n",
      "One-Class SVM  0.9598 0.986   99.15   17.45\n",
      "          GMM  0.9419 0.978   99.27   35.31\n",
      "          LOF  0.9310 0.979   99.69   36.33\n",
      "          KDE  0.7727 0.950   99.76   94.90\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 결과 비교 테이블 생성\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': method,\n",
    "        'ROC AUC': metrics['auc'],\n",
    "        'F1': metrics['f1'],\n",
    "        'TPR (%)': metrics['tpr'] * 100,\n",
    "        'FPR (%)': metrics['fpr'] * 100\n",
    "    }\n",
    "    for method, metrics in results.items()\n",
    "]).sort_values('ROC AUC', ascending=False)\n",
    "\n",
    "formatters = {\n",
    "    'ROC AUC': '{:.4f}'.format,\n",
    "    'F1': '{:.3f}'.format,\n",
    "    'TPR (%)': '{:.2f}'.format,\n",
    "    'FPR (%)': '{:.2f}'.format\n",
    "}\n",
    "\n",
    "print(comparison_df.to_string(index=False, formatters=formatters))\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d32b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ROC AUC 비교 바 차트\n",
    "plt.subplot(1, 2, 1)\n",
    "methods = comparison_df['Method'].values\n",
    "aucs = comparison_df['ROC AUC'].values\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(methods)))\n",
    "plt.barh(methods, aucs, color=colors)\n",
    "plt.xlabel('ROC AUC Score')\n",
    "plt.title('Anomaly Detection Methods Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(aucs):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
